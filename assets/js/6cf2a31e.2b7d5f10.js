"use strict";(self.webpackChunkphysical_ai_robotics=self.webpackChunkphysical_ai_robotics||[]).push([[804],{6603(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"modules/module4-vla","title":"Module 4: Vision-Language-Action (VLA)","description":"Focus: The convergence of Large Language Models and Robotics","source":"@site/docs/modules/module4-vla.md","sourceDirName":"modules","slug":"/modules/module4-vla","permalink":"/book-physical-ai-robotics/modules/module4-vla","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"courseSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/book-physical-ai-robotics/modules/module3-isaac"},"next":{"title":"Hardware Requirements","permalink":"/book-physical-ai-robotics/hardware"}}');var t=i(4848),s=i(8453);const r={sidebar_position:4},l="Module 4: Vision-Language-Action (VLA)",a={},c=[{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:2},{value:"Using OpenAI Whisper for Voice Commands",id:"using-openai-whisper-for-voice-commands",level:3},{value:"End-to-End Voice Control",id:"end-to-end-voice-control",level:3},{value:"Cognitive Planning with LLMs",id:"cognitive-planning-with-llms",level:2},{value:"Example: Natural Language to ROS Actions",id:"example-natural-language-to-ros-actions",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Multi-Modal Interaction",id:"multi-modal-interaction",level:2},{value:"Speech",id:"speech",level:3},{value:"Gesture",id:"gesture",level:3},{value:"Vision",id:"vision",level:3},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2},{value:"Project Requirements",id:"project-requirements",level:3},{value:"Technical Stack",id:"technical-stack",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Week 13 Coverage",id:"week-13-coverage",level:3},{value:"Hands-On Projects",id:"hands-on-projects",level:2},{value:"The Future of Embodied AI",id:"the-future-of-embodied-ai",level:2}];function d(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Focus"}),": The convergence of Large Language Models and Robotics"]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," models represent the next frontier in robotics:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": Understand what the robot sees"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Comprehend natural language commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Execute physical tasks in the real world"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This module brings together everything you've learned to create truly intelligent robots."}),"\n",(0,t.jsx)(n.h2,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"using-openai-whisper-for-voice-commands",children:"Using OpenAI Whisper for Voice Commands"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Whisper"})," is an open-source speech recognition model:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load Whisper model\nmodel = whisper.load_model("base")\n\n# Transcribe voice command\nresult = model.transcribe("command.mp3")\ncommand = result["text"]\n# Output: "Clean the room"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"end-to-end-voice-control",children:"End-to-End Voice Control"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[\ud83c\udfa4 Voice Input] --\x3e B[Whisper STT]\n    B --\x3e C[LLM Planner]\n    C --\x3e D[ROS 2 Actions]\n    D --\x3e E[\ud83e\udd16 Robot Execution]\n    E --\x3e F[\ud83d\udc41\ufe0f Vision Feedback]\n    F --\x3e C\n"})}),"\n",(0,t.jsx)(n.h2,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"}),"\n",(0,t.jsx)(n.p,{children:"Large Language Models (GPT-4, Claude) can translate high-level goals into actionable robot commands."}),"\n",(0,t.jsx)(n.h3,{id:"example-natural-language-to-ros-actions",children:"Example: Natural Language to ROS Actions"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Input"}),': "Clean the room"']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LLM Planning Output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "plan": [\n    {"action": "navigate", "params": {"location": "living_room"}},\n    {"action": "detect_objects", "params": {"category": "clutter"}},\n    {"action": "pick", "params": {"object_id": "toy_1"}},\n    {"action": "navigate", "params": {"location": "toy_box"}},\n    {"action": "place", "params": {"container": "toy_box"}},\n    {"action": "repeat", "params": {"until": "room_clean"}}\n  ]\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nfrom robot_controller import RobotController\n\ndef plan_with_llm(user_command: str) -> list:\n    """Convert natural language to robot action sequence"""\n    \n    system_prompt = """\n    You are a robot action planner. Convert user commands into \n    a sequence of robot actions: navigate, pick, place, detect_objects.\n    """\n    \n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": user_command}\n        ]\n    )\n    \n    return parse_action_plan(response.choices[0].message.content)\n\n# Execute plan\nrobot = RobotController()\nplan = plan_with_llm("Clean the room")\n\nfor action in plan:\n    robot.execute(action)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"multi-modal-interaction",children:"Multi-Modal Interaction"}),"\n",(0,t.jsx)(n.p,{children:"Modern robots should understand multiple input modalities:"}),"\n",(0,t.jsx)(n.h3,{id:"speech",children:"Speech"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Natural language commands"}),"\n",(0,t.jsx)(n.li,{children:'Questions ("Where is the kitchen?")'}),"\n",(0,t.jsx)(n.li,{children:'Clarifications ("Pick up the red ball, not the blue one")'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"gesture",children:"Gesture"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Pointing at objects"}),"\n",(0,t.jsx)(n.li,{children:"Hand signals for direction"}),"\n",(0,t.jsx)(n.li,{children:"Body language for intent"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"vision",children:"Vision"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Object recognition"}),"\n",(0,t.jsx)(n.li,{children:"Scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"Person tracking"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,t.jsx)(n.p,{children:"Your final project integrates all four modules:"}),"\n",(0,t.jsx)(n.h3,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Scenario"}),": A humanoid robot in a simulated home environment receives the voice command:"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:'"Go to the kitchen and bring me a cup"'}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Required Capabilities"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Speech Recognition"})," (Whisper): Transcribe the command"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"LLM Planning"})," (GPT-4): Break down into subtasks"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Navigation"})," (Nav2): Navigate to the kitchen"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Object Detection"})," (Isaac ROS): Identify the cup"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Manipulation"}),": Pick up the cup with humanoid hand"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Return Navigation"}),": Navigate back to the user"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Handoff"}),": Place the cup in the user's hand"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"technical-stack",children:"Technical Stack"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          User Voice Command             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 Whisper\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     GPT-4 Action Planning               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 ROS 2 Actions\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Robot Controller (ROS 2 + Nav2)        \u2502\n\u2502  \u2022 Navigation                           \u2502\n\u2502  \u2022 VSLAM                                \u2502\n\u2502  \u2022 Object Detection (Isaac ROS)         \u2502\n\u2502  \u2022 Manipulation                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Simulated Humanoid (Isaac Sim/Gazebo)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.h3,{id:"week-13-coverage",children:"Week 13 Coverage"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conversational AI Integration"}),": Connect GPT models to ROS 2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": Implement Whisper for voice commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": Parse intent from user speech"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal Interaction"}),": Combine speech, gesture, and vision"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-to-End System"}),": Build the complete autonomous pipeline"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-projects",children:"Hands-On Projects"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Command System"}),": Implement Whisper + GPT-4 integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Planning"}),': Translate "Make me coffee" into robot actions']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal UI"}),": Create a conversational robot interface"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Capstone Integration"}),": Complete the autonomous humanoid project"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"the-future-of-embodied-ai",children:"The Future of Embodied AI"}),"\n",(0,t.jsx)(n.p,{children:"VLA models represent the convergence of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Computer Vision"})," \u2192 Understanding the world"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Processing"})," \u2192 Understanding humans"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robotics"})," \u2192 Acting in the world"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This is the foundation of truly general-purpose robots that can operate in human environments with minimal training."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);